# Evaluation

Evaluation is research; like understanding context is. But evaluation makes the designed artefact the target of the research

We use a mix of methods, and theory. Experiments, interviews, questionnaires, observations, analytic.
The evaluation method we choose depends on what the goal of our intervention is! Different types of evaluation in different iterations of the design process.

## Inspections

Asking experts. Apply domain knowledge and theory.

### Heuristic Evaluation

A form of inspection, where the experts are HCI experts. You judge based upon compliance with accepted HCI heurstics.

### Shneiderman's Eight Golden Rules

1. Strive for Consistency
2. Enable frequent users to use shortcuts
3. Offer informative feedback
4. Design dialog to yield closure
5. Offer simple error handling
6. Permit easy reversal of actions
7. Support internal locus of control
8. Reduce short-term memory load

### Nielsen's Heuristics

1. Visibility of system status
2. Match between system and real world
3. User control and freedom
4. Consistency and standards
5. Error prevention
6. Recognition rather than recall
7. Flexibility and efficiency of use
8. Aesthetic and minimalist design
9. Help users recognize, diagnose and recover from errors
10. Help and documentation

### Process

#### Briefing

- Tell exeprts what to do
- How are issues logged, scored, ranked?

#### Evaluation Period

- Spend time with product
- Takes two passes thru the interface
- Debrief and discuss findings, compare notes, prioritise issues and discuss solutions

## Predictive Models

More formal version of Heuristics.

### Fitts' Law

The average time taken to click something is proportional to the distance from the target, divided by its size.

$MT = A + b \times ID = a + b \times log_2(\frac{2D}{W})$

## Experiments and Statistics

Lots of evaluation takes the form of experiments. i.e. Controlled comparisons between different scenarios.

Tests do two things, check for a difference between two sets of numbers and test for correlation between two sets of numbers.

For example, do people make less input errors with this new keyboard? Does the frequency of input errors decrease with experience?

## Usability Testing

A way to see how easy to use something is by testing it with real users. Users complete tasks whilst being observed.

Done in a controlled space, with emphasis on selecting representative users and developing representative tasks. Test conditions are the same for every participants. Informed consent form explains procedures.

### Verbal Protocols

Insight into cognitive activity of user, with a think-aloud protocol. User speaks out their thoughts, its easier to carry out by the evaluator.

### Software Logging

Usability analysis can involve software logging during usability testing. This is costly though.

### Experiments

Test hypothesis. Predict the relationship between two or more variables. Can be:

* Between-subjeect design
  * Different participants, with single group allocated randomly to the experimental conditions
* Within subjects design
  * All participants appear in both conditions
* Pair-wise design
  * Participants are matched in pairs

### Hawthorne Effect

The environment in which the evaluation is conducted influences or even distorts the results

## Natural Settings

Fields studies and in-the-wild studies to see how the product is used in the real world

### Field Studies

You go to them. They have real life context and interaction. It is noisy and has interruptions, and you have less control over the environment.

Aim to understand what users do naturally and how technology impacts them. Carried out freely in natural settings – “in the wild".